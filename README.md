# NLP Foundations

Collection of the most important books and (seminal) papers for the modern NLP.

## Contents

- [NLP Foundations](#nlp-foundations)
  - [Contents](#contents)
  - [Books](#books)
  - [Embeddings](#embeddings)
  - [Deep Learning](#deep-learning)
  - [Attention](#attention)
  - [Language Models](#language-models)
    - [BERT models](#bert-models)
    - [GPT models](#gpt-models)
    - [Training methodology](#training-methodology)
  - [Large Language Models](#large-language-models)
  - [Efficient Transformers](#efficient-transformers)
    - [Adapters](#adapters)
    - [Low-rank adaptations](#low-rank-adaptations)
    - [Small (Large) Language Models](#small-large-language-models)
  - [Other](#other)

## Books

1. [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/) [^1]
2. [Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch) [^2]

[^1]: Excellent introduction to NLP.
[^2]: Excellent, in-depth introduction to modern decoder-based large language models.

## Embeddings

TODO

## Deep Learning

1. [Learning phrase representations using RNN encoder-decoder for statistical machine translation](https://arxiv.org/abs/1406.1078)
2. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)

## Attention

1. [Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473)
2. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Transformers paper)

## Language Models

1. [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)
2. [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/abs/2107.13586)

### BERT models

1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
2. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
3. [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
4. [DistilBERT, A distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
5. [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957)
6. [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)
7. [MobileBERT: A Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)
8. [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)
9. [MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers](https://arxiv.org/abs/2012.15828)
10. [DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing](https://arxiv.org/abs/2111.09543)

### GPT models

1. [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) (GPT-1)
2. [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (GPT-2)
3. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3)
4. [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (InstructGPT)

### Training methodology

1. [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) (XLM)
2. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
3. [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) (XLM-RoBERTa)

## Large Language Models

1. [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
2. [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
3. [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)

## Efficient Transformers

1. [Efficiently Scaling Transformer Inference](https://arxiv.org/abs/2211.05102)

### Adapters

TODO

### Low-rank adaptations

1. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
2. [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
3. [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353)
4. [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354)

### Small (Large) Language Models

1. [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385)
2. [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)

## Other

1. [On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)
2. [On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)
